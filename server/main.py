#!/usr/bin/env python3
"""
MCP Tekla+ ä¼ºæœå™¨ç«¯ä¸»ç¨‹å¼
é‹è¡Œ DeepSeek-Coder-v2 16B å’Œ RAG ç³»çµ±
"""

import asyncio
import logging
import os
from contextlib import asynccontextmanager
from typing import Dict, List, Optional

import torch
import uvicorn
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from services.ai_service import AIService
from services.rag_service import RAGService
from services.tekla_knowledge import TeklaKnowledgeBase
from utils.gpu_monitor import GPUMonitor
from utils.logger import setup_logger

# è¨­ç½®æ—¥èªŒ
logger = setup_logger(__name__)

# å…¨åŸŸæœå‹™å¯¦ä¾‹
ai_service: Optional[AIService] = None
rag_service: Optional[RAGService] = None
tekla_kb: Optional[TeklaKnowledgeBase] = None
gpu_monitor: Optional[GPUMonitor] = None

# WebSocket é€£æ¥ç®¡ç†
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
    
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        logger.info(f"WebSocket é€£æ¥å»ºç«‹ï¼Œç•¶å‰é€£æ¥æ•¸: {len(self.active_connections)}")
    
    def disconnect(self, websocket: WebSocket):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
        logger.info(f"WebSocket é€£æ¥æ–·é–‹ï¼Œç•¶å‰é€£æ¥æ•¸: {len(self.active_connections)}")
    
    async def broadcast(self, message: dict):
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except Exception as e:
                logger.error(f"å»£æ’­è¨Šæ¯å¤±æ•—: {e}")

manager = ConnectionManager()

# è«‹æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    message: str
    context: Optional[str] = None
    use_rag: bool = True
    temperature: float = 0.7
    max_tokens: int = 2048

class RAGQueryRequest(BaseModel):
    query: str
    top_k: int = 5
    threshold: float = 0.7

class TeklaCommandRequest(BaseModel):
    command: str
    parameters: Optional[Dict] = None
    context: Optional[str] = None

# æ‡‰ç”¨ç¨‹å¼ç”Ÿå‘½é€±æœŸç®¡ç†
@asynccontextmanager
async def lifespan(app: FastAPI):
    """æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•å’Œé—œé–‰æ™‚çš„è™•ç†"""
    global ai_service, rag_service, tekla_kb, gpu_monitor
    
    logger.info("ğŸš€ å•Ÿå‹• MCP Tekla+ ä¼ºæœå™¨...")
    
    try:
        # åˆå§‹åŒ– GPU ç›£æ§
        gpu_monitor = GPUMonitor()
        logger.info(f"GPU ç›£æ§å·²å•Ÿå‹•ï¼Œæª¢æ¸¬åˆ° {gpu_monitor.get_gpu_count()} å€‹ GPU")
        
        # åˆå§‹åŒ– Tekla çŸ¥è­˜åº«
        logger.info("ğŸ“š åˆå§‹åŒ– Tekla çŸ¥è­˜åº«...")
        tekla_kb = TeklaKnowledgeBase()
        await tekla_kb.initialize()
        
        # åˆå§‹åŒ– RAG æœå‹™
        logger.info("ğŸ” åˆå§‹åŒ– RAG æœå‹™...")
        rag_service = RAGService(tekla_kb)
        await rag_service.initialize()
        
        # åˆå§‹åŒ– AI æœå‹™
        logger.info("ğŸ¤– è¼‰å…¥ DeepSeek-Coder-v2 16B æ¨¡å‹...")
        ai_service = AIService(
            model_name="deepseek-ai/deepseek-coder-6.7b-instruct",  # å¯èª¿æ•´ç‚º 16B ç‰ˆæœ¬
            device_map="auto",
            torch_dtype=torch.float16
        )
        await ai_service.initialize()
        
        logger.info("âœ… æ‰€æœ‰æœå‹™åˆå§‹åŒ–å®Œæˆ")
        
        yield
        
    except Exception as e:
        logger.error(f"âŒ æœå‹™åˆå§‹åŒ–å¤±æ•—: {e}")
        raise
    finally:
        logger.info("ğŸ”„ é—œé–‰æœå‹™...")
        if ai_service:
            await ai_service.cleanup()
        if rag_service:
            await rag_service.cleanup()
        if tekla_kb:
            await tekla_kb.cleanup()
        logger.info("âœ… æœå‹™é—œé–‰å®Œæˆ")

# å‰µå»º FastAPI æ‡‰ç”¨ç¨‹å¼
app = FastAPI(
    title="MCP Tekla+ AI ä¼ºæœå™¨",
    description="DeepSeek-Coder-v2 16B + RAG ç³»çµ±",
    version="1.0.0",
    lifespan=lifespan
)

# CORS è¨­ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿç”¢ç’°å¢ƒæ‡‰é™åˆ¶ä¾†æº
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å¥åº·æª¢æŸ¥ç«¯é»
@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    gpu_info = gpu_monitor.get_gpu_status() if gpu_monitor else {}
    
    return {
        "status": "healthy",
        "services": {
            "ai_service": ai_service is not None and ai_service.is_ready(),
            "rag_service": rag_service is not None and rag_service.is_ready(),
            "tekla_kb": tekla_kb is not None and tekla_kb.is_ready()
        },
        "gpu_status": gpu_info,
        "timestamp": asyncio.get_event_loop().time()
    }

# AI èŠå¤©ç«¯é»
@app.post("/api/chat")
async def chat(request: ChatRequest):
    """AI èŠå¤©ç«¯é»"""
    if not ai_service or not ai_service.is_ready():
        raise HTTPException(status_code=503, detail="AI æœå‹™æœªå°±ç·’")
    
    try:
        # å¦‚æœå•Ÿç”¨ RAGï¼Œå…ˆæª¢ç´¢ç›¸é—œçŸ¥è­˜
        context = request.context or ""
        if request.use_rag and rag_service and rag_service.is_ready():
            rag_results = await rag_service.query(
                request.message, 
                top_k=5, 
                threshold=0.7
            )
            if rag_results:
                context += "\n\nç›¸é—œ Tekla çŸ¥è­˜:\n" + "\n".join([
                    f"- {result['content']}" for result in rag_results
                ])
        
        # ç”Ÿæˆå›æ‡‰
        response = await ai_service.generate_response(
            message=request.message,
            context=context,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        
        # å»£æ’­åˆ°æ‰€æœ‰é€£æ¥çš„å®¢æˆ¶ç«¯
        await manager.broadcast({
            "type": "chat_response",
            "message": request.message,
            "response": response,
            "timestamp": asyncio.get_event_loop().time()
        })
        
        return {
            "response": response,
            "context_used": bool(context),
            "rag_enabled": request.use_rag
        }
        
    except Exception as e:
        logger.error(f"èŠå¤©è™•ç†éŒ¯èª¤: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# RAG æŸ¥è©¢ç«¯é»
@app.post("/api/rag/query")
async def rag_query(request: RAGQueryRequest):
    """RAG çŸ¥è­˜æŸ¥è©¢ç«¯é»"""
    if not rag_service or not rag_service.is_ready():
        raise HTTPException(status_code=503, detail="RAG æœå‹™æœªå°±ç·’")
    
    try:
        results = await rag_service.query(
            request.query,
            top_k=request.top_k,
            threshold=request.threshold
        )
        
        return {
            "query": request.query,
            "results": results,
            "count": len(results)
        }
        
    except Exception as e:
        logger.error(f"RAG æŸ¥è©¢éŒ¯èª¤: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Tekla å‘½ä»¤ç«¯é»
@app.post("/api/tekla/command")
async def tekla_command(request: TeklaCommandRequest):
    """Tekla å‘½ä»¤è™•ç†ç«¯é»"""
    if not ai_service or not ai_service.is_ready():
        raise HTTPException(status_code=503, detail="AI æœå‹™æœªå°±ç·’")
    
    try:
        # ä½¿ç”¨ RAG ç²å–ç›¸é—œ Tekla çŸ¥è­˜
        context = request.context or ""
        if rag_service and rag_service.is_ready():
            rag_results = await rag_service.query(
                f"Tekla {request.command}",
                top_k=3,
                threshold=0.8
            )
            if rag_results:
                context += "\n\nTekla API åƒè€ƒ:\n" + "\n".join([
                    f"- {result['content']}" for result in rag_results
                ])
        
        # ç”Ÿæˆ Tekla å‘½ä»¤ä»£ç¢¼
        prompt = f"""
        æ ¹æ“šä»¥ä¸‹è¦æ±‚ç”Ÿæˆ Tekla Structures 2025 API ä»£ç¢¼:
        
        å‘½ä»¤: {request.command}
        åƒæ•¸: {request.parameters or 'ç„¡'}
        
        {context}
        
        è«‹ç”Ÿæˆå®Œæ•´çš„ C# ä»£ç¢¼ï¼ŒåŒ…å«å¿…è¦çš„ using èªå¥å’ŒéŒ¯èª¤è™•ç†ã€‚
        """
        
        response = await ai_service.generate_response(
            message=prompt,
            temperature=0.3,  # è¼ƒä½æº«åº¦ç¢ºä¿ä»£ç¢¼æº–ç¢ºæ€§
            max_tokens=1024
        )
        
        return {
            "command": request.command,
            "parameters": request.parameters,
            "generated_code": response,
            "context_used": bool(context)
        }
        
    except Exception as e:
        logger.error(f"Tekla å‘½ä»¤è™•ç†éŒ¯èª¤: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# GPU ç‹€æ…‹ç«¯é»
@app.get("/api/gpu/status")
async def gpu_status():
    """GPU ç‹€æ…‹æŸ¥è©¢ç«¯é»"""
    if not gpu_monitor:
        raise HTTPException(status_code=503, detail="GPU ç›£æ§æœªå•Ÿå‹•")
    
    return gpu_monitor.get_detailed_status()

# WebSocket ç«¯é»
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket é€£æ¥ç«¯é»"""
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_json()
            
            # è™•ç†ä¸åŒé¡å‹çš„ WebSocket è¨Šæ¯
            if data.get("type") == "ping":
                await websocket.send_json({"type": "pong"})
            elif data.get("type") == "gpu_status":
                if gpu_monitor:
                    status = gpu_monitor.get_gpu_status()
                    await websocket.send_json({
                        "type": "gpu_status",
                        "data": status
                    })
            
    except WebSocketDisconnect:
        manager.disconnect(websocket)
    except Exception as e:
        logger.error(f"WebSocket éŒ¯èª¤: {e}")
        manager.disconnect(websocket)

if __name__ == "__main__":
    # æª¢æŸ¥ GPU å¯ç”¨æ€§
    if not torch.cuda.is_available():
        logger.warning("âš ï¸ æœªæª¢æ¸¬åˆ° CUDA GPUï¼Œå°‡ä½¿ç”¨ CPU é‹è¡Œ")
    else:
        gpu_count = torch.cuda.device_count()
        logger.info(f"ğŸ® æª¢æ¸¬åˆ° {gpu_count} å€‹ GPU")
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            logger.info(f"  GPU {i}: {gpu_name}")
    
    # å•Ÿå‹•ä¼ºæœå™¨
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=False,  # ç”Ÿç”¢ç’°å¢ƒé—œé–‰è‡ªå‹•é‡è¼‰
        workers=1,     # ç”±æ–¼æ¨¡å‹è¼‰å…¥ï¼Œä½¿ç”¨å–®ä¸€ worker
        log_level="info"
    )
